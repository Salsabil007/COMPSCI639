{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Homework #5.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 639 - Foundations of Data Science\n",
    "\n",
    "# Homework \\#5\n",
    "\n",
    "\n",
    "In this final homework, you will get to practice what you have learned about (linear) regression and classification and connect it to topics you learned about before (such as MLE, bootstrap, and confidence intervals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 1: Least Squares as MLE [$\\color{blue}{\\text{Extra Credit, 20pts}}$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned in class that under suitable assumptions, the estimates we get for the coefficients $\\beta$ in linear regression using least squares can also be viewed as maximum likelihood estimates. In this question, you are asked to prove this claim. The setup is as follows. Assume you are given $n$ pairs of feature vectors $x_i$ (of length $p \\geq 1$) and their corresponding responses/labels $y_i$. Vectors $x_1, x_2, \\dots, x_n$ are drawn i.i.d. from some underlying distribution. Response variables $y_1, y_2, \\dots, y_n$ are related to the feature vectors via\n",
    "\n",
    "$$\n",
    "    y_i = \\beta_0 + \\beta_1 [x_i]_1 + \\beta_2 [x_i]_2 + \\dots + \\beta_p [x_i]_p + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i$ are random errors drawn i.i.d. from $\\mathcal{N}(0, \\sigma^2)$ ($\\sigma^2$ is unknown), independently of $x_i$. As discussed in class, to perform linear regression, you want to estimate the coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p.$\n",
    "\n",
    "1. When $x_i$ is given (i.e., conditioning on $x_i$), what is the distribution of $y_i$?\n",
    "2. What are the likelihood and the log-likelihood function? (You can write these functions conditioned on $x_1, \\dots, x_n$ or not; your final answer should not get affected.)\n",
    "3. Argue that maximizing the log-likelihood function is the same as minimizing the residual sum of squares (RSS), and, thus, least squares estimates of the coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ are also maximum likelihood estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 2: Comparing Two Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the `lin_reg.csv` file that contains 4 columns and 100 rows. Each row has an independent sample of the values of three features $X_1, X_2, X_3$ followed by the value of a response $Y$ corresponding to those feature values. You are asked to train and evaluate two linear regression models:\n",
    "\n",
    "$$\n",
    "    f_1(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    f_2(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 {X_2}^2 + \\beta_3 {X_3}^3.\n",
    "$$\n",
    "\n",
    "You can use `numpy.linalg.lstsq` (https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) to compute the coefficients for these two models. Display clearly which value corresponds to which coefficient in your output. (You will need to add a column of ones in your array to compute `beta_0`; You can use `numpy.insert(arr, 0, 1, axis=1)` to insert one column of ones into your array.)\n",
    "\n",
    "Using the techniques discussed in class, assess the accuracy of the two models and discuss which one of the two models you would use to make predictions and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-151.03190508   -9.84642241   -9.25205485   69.79907781]\n",
      "[3862389.40874953]\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "3862389.4087495315\n",
      "196.52962648795554\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"lin_reg.csv\")\n",
    "#print(data)\n",
    "xtrain = data\n",
    "xtrain = xtrain.to_numpy()\n",
    "y = xtrain[:, 3]\n",
    "xtrain = np.delete(xtrain, 3, axis=1)\n",
    "xtrain = np.insert(xtrain,0,1,axis = 1)\n",
    "a,b,c,d = np.linalg.lstsq(xtrain, y, rcond=None)\n",
    "print(a)\n",
    "print(b)\n",
    "a = a.reshape(a.shape[0], -1)\n",
    "\n",
    "ar = np.matmul(xtrain , a)\n",
    "print(ar.shape)\n",
    "y = y.reshape(y.shape[0], -1)\n",
    "print(y.shape)\n",
    "\n",
    "x = y - ar\n",
    "x = np.square(x)\n",
    "x = np.sum(x)\n",
    "print(x) ##RSS/MSE\n",
    "RSE = x / xtrain.shape[0]\n",
    "RSE = np.sqrt(RSE)\n",
    "print(RSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.52380685 -3.10041619 -0.49954944  0.20016408]\n",
      "[536.27736962]\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "536.2773696191174\n",
      "2.3157663302222815\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"lin_reg.csv\")\n",
    "data['X_2']=np.power((data['X_2']),2)\n",
    "data['X_3']=np.power((data['X_3']),3)\n",
    "xtrain = data\n",
    "xtrain = xtrain.to_numpy()\n",
    "y = xtrain[:, 3]\n",
    "xtrain = np.delete(xtrain, 3, axis=1)\n",
    "xtrain = np.insert(xtrain,0,1,axis = 1)\n",
    "a,b,c,d = np.linalg.lstsq(xtrain, y, rcond=None)\n",
    "print(a)\n",
    "print(b)\n",
    "a = a.reshape(a.shape[0], -1)\n",
    "\n",
    "ar = np.matmul(xtrain , a)\n",
    "print(ar.shape)\n",
    "y = y.reshape(y.shape[0], -1)\n",
    "print(y.shape)\n",
    "\n",
    "x = y - ar\n",
    "x = np.square(x)\n",
    "x = np.sum(x)\n",
    "print(x) ##RSS/MSE\n",
    "RSE = x / xtrain.shape[0]\n",
    "RSE = np.sqrt(RSE)\n",
    "print(RSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  2.  2.]\n",
      " [ 1. 11.  5.  9.]\n",
      " [ 1.  8. 19.  6.]\n",
      " [ 1. 19.  1. 18.]\n",
      " [ 1.  5. 13. 20.]\n",
      " [ 1. 12. 16. 11.]\n",
      " [ 1. 17. 14. 16.]\n",
      " [ 1.  8.  1.  0.]\n",
      " [ 1. 11. 14. 10.]\n",
      " [ 1. 12. 13. 16.]\n",
      " [ 1.  5. 17.  5.]\n",
      " [ 1.  7.  7.  0.]\n",
      " [ 1.  5. 10.  5.]\n",
      " [ 1.  4. 16. 16.]\n",
      " [ 1. 11. 16. 17.]\n",
      " [ 1.  5. 14. 13.]\n",
      " [ 1. 16. 11. 18.]\n",
      " [ 1. 11. 11. 14.]\n",
      " [ 1.  5. 12. 14.]\n",
      " [ 1. 20. 16.  7.]\n",
      " [ 1. 15.  8. 15.]\n",
      " [ 1. 16. 16. 11.]\n",
      " [ 1. 14. 14. 11.]\n",
      " [ 1. 18. 17. 14.]\n",
      " [ 1. 15.  7. 10.]\n",
      " [ 1.  5. 19.  8.]\n",
      " [ 1. 15.  9.  9.]\n",
      " [ 1. 16. 17. 16.]\n",
      " [ 1. 16. 20. 19.]\n",
      " [ 1. 18. 13.  9.]\n",
      " [ 1.  6. 15. 16.]\n",
      " [ 1. 11. 19.  2.]\n",
      " [ 1. 10.  0.  6.]\n",
      " [ 1.  3.  1. 18.]\n",
      " [ 1. 20.  1.  8.]\n",
      " [ 1. 18.  7.  3.]\n",
      " [ 1. 16.  4.  8.]\n",
      " [ 1.  7.  6.  1.]\n",
      " [ 1. 13.  1.  1.]\n",
      " [ 1. 11. 11.  5.]\n",
      " [ 1.  7.  0.  2.]\n",
      " [ 1.  3.  2.  0.]\n",
      " [ 1.  1.  0. 11.]\n",
      " [ 1.  8.  4.  5.]\n",
      " [ 1.  5. 16.  0.]\n",
      " [ 1. 12. 18.  1.]\n",
      " [ 1.  7.  4.  1.]\n",
      " [ 1.  0. 11. 19.]\n",
      " [ 1. 20.  3.  9.]\n",
      " [ 1. 10. 15.  0.]\n",
      " [ 1.  9. 14. 17.]\n",
      " [ 1. 19.  1.  8.]\n",
      " [ 1. 12. 19.  4.]\n",
      " [ 1. 15.  7.  2.]\n",
      " [ 1. 10.  3.  0.]\n",
      " [ 1. 14.  4. 16.]\n",
      " [ 1. 18. 12. 15.]\n",
      " [ 1. 16. 10.  4.]\n",
      " [ 1. 10.  8.  8.]\n",
      " [ 1. 19. 13. 20.]\n",
      " [ 1.  0. 17.  4.]\n",
      " [ 1.  1.  8.  1.]\n",
      " [ 1.  4.  5.  5.]\n",
      " [ 1.  3. 14. 20.]\n",
      " [ 1.  7. 16.  1.]\n",
      " [ 1.  7.  7. 14.]\n",
      " [ 1.  2.  8.  2.]\n",
      " [ 1. 18.  7. 19.]\n",
      " [ 1. 19. 11.  8.]\n",
      " [ 1. 13.  8. 16.]\n",
      " [ 1.  0.  4.  1.]\n",
      " [ 1. 12. 13.  5.]\n",
      " [ 1.  3. 16.  2.]\n",
      " [ 1.  7.  3.  3.]\n",
      " [ 1.  0.  5.  7.]\n",
      " [ 1.  3.  6.  0.]\n",
      " [ 1. 16. 14. 14.]\n",
      " [ 1.  9. 17. 20.]\n",
      " [ 1. 12.  6.  6.]\n",
      " [ 1. 13. 13. 16.]\n",
      " [ 1.  0. 18. 18.]\n",
      " [ 1.  1. 13. 16.]\n",
      " [ 1. 18.  5.  3.]\n",
      " [ 1. 15. 11.  0.]\n",
      " [ 1. 16.  3. 19.]\n",
      " [ 1. 11.  9. 11.]\n",
      " [ 1.  9.  0. 13.]\n",
      " [ 1.  3.  3.  9.]\n",
      " [ 1.  6.  0. 14.]\n",
      " [ 1.  1. 13. 20.]\n",
      " [ 1. 15. 14.  6.]\n",
      " [ 1. 18. 19.  2.]\n",
      " [ 1.  0.  9.  0.]\n",
      " [ 1. 11.  9.  2.]\n",
      " [ 1.  7. 15.  6.]\n",
      " [ 1.  3. 18. 11.]\n",
      " [ 1. 12. 14.  4.]\n",
      " [ 1. 11. 12.  3.]\n",
      " [ 1.  8.  3.  3.]\n",
      " [ 1.  2. 19. 10.]]\n",
      "[[ 1.  1.  2.  2.]\n",
      " [11.  1.  5.  9.]\n",
      " [ 8.  1. 19.  6.]\n",
      " [19.  1.  1. 18.]\n",
      " [ 5.  1. 13. 20.]\n",
      " [12.  1. 16. 11.]\n",
      " [17.  1. 14. 16.]\n",
      " [ 8.  1.  1.  0.]\n",
      " [11.  1. 14. 10.]\n",
      " [12.  1. 13. 16.]\n",
      " [ 5.  1. 17.  5.]\n",
      " [ 7.  1.  7.  0.]\n",
      " [ 5.  1. 10.  5.]\n",
      " [ 4.  1. 16. 16.]\n",
      " [11.  1. 16. 17.]\n",
      " [ 5.  1. 14. 13.]\n",
      " [16.  1. 11. 18.]\n",
      " [11.  1. 11. 14.]\n",
      " [ 5.  1. 12. 14.]\n",
      " [20.  1. 16.  7.]\n",
      " [15.  1.  8. 15.]\n",
      " [16.  1. 16. 11.]\n",
      " [14.  1. 14. 11.]\n",
      " [18.  1. 17. 14.]\n",
      " [15.  1.  7. 10.]\n",
      " [ 5.  1. 19.  8.]\n",
      " [15.  1.  9.  9.]\n",
      " [16.  1. 17. 16.]\n",
      " [16.  1. 20. 19.]\n",
      " [18.  1. 13.  9.]\n",
      " [ 6.  1. 15. 16.]\n",
      " [11.  1. 19.  2.]\n",
      " [10.  1.  0.  6.]\n",
      " [ 3.  1.  1. 18.]\n",
      " [20.  1.  1.  8.]\n",
      " [18.  1.  7.  3.]\n",
      " [16.  1.  4.  8.]\n",
      " [ 7.  1.  6.  1.]\n",
      " [13.  1.  1.  1.]\n",
      " [11.  1. 11.  5.]\n",
      " [ 7.  1.  0.  2.]\n",
      " [ 3.  1.  2.  0.]\n",
      " [ 1.  1.  0. 11.]\n",
      " [ 8.  1.  4.  5.]\n",
      " [ 5.  1. 16.  0.]\n",
      " [12.  1. 18.  1.]\n",
      " [ 7.  1.  4.  1.]\n",
      " [ 0.  1. 11. 19.]\n",
      " [20.  1.  3.  9.]\n",
      " [10.  1. 15.  0.]\n",
      " [ 9.  1. 14. 17.]\n",
      " [19.  1.  1.  8.]\n",
      " [12.  1. 19.  4.]\n",
      " [15.  1.  7.  2.]\n",
      " [10.  1.  3.  0.]\n",
      " [14.  1.  4. 16.]\n",
      " [18.  1. 12. 15.]\n",
      " [16.  1. 10.  4.]\n",
      " [10.  1.  8.  8.]\n",
      " [19.  1. 13. 20.]\n",
      " [ 0.  1. 17.  4.]\n",
      " [ 1.  1.  8.  1.]\n",
      " [ 4.  1.  5.  5.]\n",
      " [ 3.  1. 14. 20.]\n",
      " [ 7.  1. 16.  1.]\n",
      " [ 7.  1.  7. 14.]\n",
      " [ 2.  1.  8.  2.]\n",
      " [18.  1.  7. 19.]\n",
      " [19.  1. 11.  8.]\n",
      " [13.  1.  8. 16.]\n",
      " [ 0.  1.  4.  1.]\n",
      " [12.  1. 13.  5.]\n",
      " [ 3.  1. 16.  2.]\n",
      " [ 7.  1.  3.  3.]\n",
      " [ 0.  1.  5.  7.]\n",
      " [ 3.  1.  6.  0.]\n",
      " [16.  1. 14. 14.]\n",
      " [ 9.  1. 17. 20.]\n",
      " [12.  1.  6.  6.]\n",
      " [13.  1. 13. 16.]\n",
      " [ 0.  1. 18. 18.]\n",
      " [ 1.  1. 13. 16.]\n",
      " [18.  1.  5.  3.]\n",
      " [15.  1. 11.  0.]\n",
      " [16.  1.  3. 19.]\n",
      " [11.  1.  9. 11.]\n",
      " [ 9.  1.  0. 13.]\n",
      " [ 3.  1.  3.  9.]\n",
      " [ 6.  1.  0. 14.]\n",
      " [ 1.  1. 13. 20.]\n",
      " [15.  1. 14.  6.]\n",
      " [18.  1. 19.  2.]\n",
      " [ 0.  1.  9.  0.]\n",
      " [11.  1.  9.  2.]\n",
      " [ 7.  1. 15.  6.]\n",
      " [ 3.  1. 18. 11.]\n",
      " [12.  1. 14.  4.]\n",
      " [11.  1. 12.  3.]\n",
      " [ 8.  1.  3.  3.]\n",
      " [ 2.  1. 19. 10.]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"lin_reg.csv\")\n",
    "xtrain = data\n",
    "xtrain = xtrain.to_numpy()\n",
    "y = xtrain[:, 0]\n",
    "\n",
    "\n",
    "xtrain = np.delete(xtrain, 3, axis=1)\n",
    "xtrain = np.insert(xtrain,0,1,axis = 1)\n",
    "print(xtrain)\n",
    "xtrain[:, [1, 0]] = xtrain[:, [0, 1]]\n",
    "print(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 3: Classifying Apples and Oranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 12(a)-(d) in Chapter 4 of the Intro to Stat Learning book (https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Problem 4: Bootstrapping Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to use bootstrapping to estimate the median of the US Family income in the previous homework. This time we are going to apply the same concept, but instead we will estimate the nitric oxides concentration inside houses in Boston. We will use the data from Boston Housing Dataset `housing.csv` for this question. **Specifically in this question you will use bootstrap to make predictions for the nitric oxides concentration based on the index of accessibility to radial highways (column name `RAD`), the proportion of non-retail business acres per town (column name `INDUS`) and the proportion of owner-occupied units built prior to 1940 (column name `AGE`).** \n",
    "\n",
    "You can run the below cell to load the data and visualize the table after selection the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOX</th>\n",
       "      <th>RAD</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.538</td>\n",
       "      <td>1</td>\n",
       "      <td>2.31</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.469</td>\n",
       "      <td>2</td>\n",
       "      <td>7.07</td>\n",
       "      <td>78.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469</td>\n",
       "      <td>2</td>\n",
       "      <td>7.07</td>\n",
       "      <td>61.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.458</td>\n",
       "      <td>3</td>\n",
       "      <td>2.18</td>\n",
       "      <td>45.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.458</td>\n",
       "      <td>3</td>\n",
       "      <td>2.18</td>\n",
       "      <td>54.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.573</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93</td>\n",
       "      <td>69.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.573</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93</td>\n",
       "      <td>76.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.573</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.573</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93</td>\n",
       "      <td>89.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.573</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NOX  RAD  INDUS   AGE\n",
       "0    0.538    1   2.31  65.2\n",
       "1    0.469    2   7.07  78.9\n",
       "2    0.469    2   7.07  61.1\n",
       "3    0.458    3   2.18  45.8\n",
       "4    0.458    3   2.18  54.2\n",
       "..     ...  ...    ...   ...\n",
       "501  0.573    1  11.93  69.1\n",
       "502  0.573    1  11.93  76.7\n",
       "503  0.573    1  11.93  91.0\n",
       "504  0.573    1  11.93  89.3\n",
       "505  0.573    1  11.93  80.8\n",
       "\n",
       "[506 rows x 4 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"housing.csv\", delim_whitespace=True)\n",
    "data = data[[\"NOX\", \"RAD\", \"INDUS\", \"AGE\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following model for the NOX concentration given the three input variables (**note the squared term for `AGE`**).\n",
    "$$\n",
    "    f(X) = \\beta_0 + \\beta_1 X_{INDUS} + \\beta_2 X_{RAD} + \\beta_3 X_{AGE}^2.\n",
    "$$\n",
    "\n",
    "1. Similar to Problem 2, implement the function `linear_regression()` using `numpy.linalg.lstsq` to compute the coefficients of our linear regression model. Run the cell below the function to output the coefficients of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_regression(data):\n",
    "    #print(data)\n",
    "    xtrain = data\n",
    "    xtrain['AGE']=xtrain['AGE']*xtrain['AGE'] #np.power((xtrain['AGE']),2)\n",
    "    \n",
    "    xtrain = xtrain.to_numpy()\n",
    "    \n",
    "    y = xtrain[:, 0]\n",
    "    #print(y)\n",
    "    xtrain = np.delete(xtrain, 0, axis=1)\n",
    "    #xtrain[:, [1, 0]] = xtrain[:, [0, 1]]\n",
    "    \n",
    "    xtrain = np.insert(xtrain,0,1,axis = 1)\n",
    "    #print(xtrain)\n",
    "    a,b,c,d = np.linalg.lstsq(xtrain, y, rcond=None)\n",
    "    #print(a)\n",
    "    data['AGE'] = np.sqrt(data['AGE'])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.84897306e-01 2.63546889e-03 6.17524345e-03 1.38092107e-05]\n",
      "       NOX  RAD  INDUS   AGE\n",
      "0    0.538    1   2.31  65.2\n",
      "1    0.469    2   7.07  78.9\n",
      "2    0.469    2   7.07  61.1\n",
      "3    0.458    3   2.18  45.8\n",
      "4    0.458    3   2.18  54.2\n",
      "..     ...  ...    ...   ...\n",
      "501  0.573    1  11.93  69.1\n",
      "502  0.573    1  11.93  76.7\n",
      "503  0.573    1  11.93  91.0\n",
      "504  0.573    1  11.93  89.3\n",
      "505  0.573    1  11.93  80.8\n",
      "\n",
      "[506 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "beta = linear_regression(data)\n",
    "print(beta)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>p4-1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "p4-1 results: All test cases passed!"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"p4-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of regression is to make predictions for a new data point which is not already part of the original samples, but is drawn from the same distribution. In our case, suppose we want to predict the NOX concentration of a housing unit which was not included in the original data set.\n",
    "\n",
    "Our new housing unit has the following data ([`RAD`, `INDUS`, `AGE`]) : `X = [3, 5.5, 60]`.\n",
    "\n",
    "2. Write a function that predicts the NOX concentration level based on our model. Use this function to output the prediction for the NOX level of our new housing unit, based on the coefficients you have computed in the Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(beta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta: np.array\n",
    "        beta = np.array([beta_0, beta_1, beta_2, beta_3]). The coeffs of our model.\n",
    "    X: np.array\n",
    "        X = [X]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Predicted NOX concentration level.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = beta[0] + beta[2] * X[1] + beta[1] * X[0] + X[2] * X[2] * beta[3]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>p4-2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "p4-2 results: All test cases passed!"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"p4-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can simulate new samples by randomly sampling with replacement from the original sample, as many times as the original sample size. Write a function that samples with replacement the same number of samples from the original dataframe, and returns a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap_sampling(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.Dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Dataframe\n",
    "        A resampled data from original data\n",
    "    \"\"\"\n",
    "    \n",
    "    our_sample = data.sample(n=len(data), replace=True)\n",
    "    return our_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>p4-3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "p4-3 results: All test cases passed!"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"p4-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "As data scientists, we know that had the samples been different, the regression results would have been different too, so would our prediction. To see how good our prediction is, we must get a sense of how variable the prediction can be. This is where bootstrapping comes in handy: **We make many predictions using the bootstrapped model coefficents.**\n",
    "\n",
    "4. Call the your function `bootstrap_sampling()` above to simulate new samples and use `linear_regression()` to get the new coefficients for your new samples, then call your function `predict()` to output the prediction of NOX level for the new housing unit.\n",
    "\n",
    "5. Repeat the steps in Part 4 for $N = 10000$ times, and output a list of boostrapped predictions. Your list should only contain the $N$ outcomes of NOX level of the new housing unit. (Note: This part should take less than a minute to run, if your `bootstrap_sampling()` method is implemented efficiently.)\n",
    "\n",
    "6. Based on the original prediction of $\\bar{x}$ you made in Part 2, plot the variability of bootstrapped predictions against $\\bar{x}$ in a historgramm, i.e. $x - \\bar{x}$. Based on this result, use the empirical bootstrap method to compute a 80% confidence interval for the prediction. (**DO NOT** use the bootstrap percentile method.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "#temp = data\n",
    "\n",
    "for i in np.arange(10000):\n",
    "    sample = bootstrap_sampling(data)\n",
    "    beta = linear_regression(sample) ##which is the test data to make prediction?\n",
    "    sample = sample.drop(['NOX'], axis = 1)\n",
    "    #print(len(sample))\n",
    "    for index, row in sample.iterrows():\n",
    "        pred.append(predict(beta, row.to_numpy()))\n",
    "        \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Boston Data Sunday Magazine has a weekly article commenting on housing units around Boston and gives their recommendation of whether an housing unit is a good purchase or not (assume binary recommendation, either one (\"buy\") or zero (\"no-buy\")). Fortunately we have gained some insider information and understand that they make recommendations based on `AGE` (proportion of owner-occupied units built prior to 1940), `DIS` (weighted distances to ﬁve Boston employment centers), `RAD` (index of accessibility to radial highways) and `TAX` (full-value property-tax rate per $10,000).\n",
    "\n",
    "Suppose their binary buy/no-buy recommendations for the 506 housing units in the `housing.csv` is given below as variable `recommend_buy_data`. We want to model their recommmendation as a logistic regression problem, where the probability of recommending \"buy\" is given by\n",
    "$$\n",
    "    \\mathbb{P} [\\text{Recommend Buy} | X] = \\frac{e^{\\gamma_0 + \\gamma_1 X_{AGE} + \\gamma_2 X_{DIS} + \\gamma_3 X_{RAD} + \\gamma_4 X_{TAX}}}{ 1 + e^{\\gamma_0 + \\gamma_1 X_{AGE} + \\gamma_2 X_{DIS} + \\gamma_3 X_{RAD} + \\gamma_4 X_{TAX}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AGE     DIS  RAD    TAX\n",
       "0    65.2  4.0900    1  296.0\n",
       "1    78.9  4.9671    2  242.0\n",
       "2    61.1  4.9671    2  242.0\n",
       "3    45.8  6.0622    3  222.0\n",
       "4    54.2  6.0622    3  222.0\n",
       "..    ...     ...  ...    ...\n",
       "501  69.1  2.4786    1  273.0\n",
       "502  76.7  2.2875    1  273.0\n",
       "503  91.0  2.1675    1  273.0\n",
       "504  89.3  2.3889    1  273.0\n",
       "505  80.8  2.5050    1  273.0\n",
       "\n",
       "[506 rows x 4 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_buy_data = [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "data = pd.read_csv(\"housing.csv\", delim_whitespace=True)\n",
    "data = data[[\"AGE\", \"DIS\", \"RAD\", \"TAX\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Conduct a logistic regression using `sklearn.linear_model.LogisticRegression` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), and estimate the probability that Boston Data Sunday Magazine would recomendation buying this housing unit with the following data: `[AGE, DIS, RAD, TAX] = [30, 4.5, 3, 230]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 4)\n",
      "506\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7727272727272727"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(data, recommend_buy_data)\n",
    "print(data.shape)\n",
    "print(len(recommend_buy_data))\n",
    "a = clf.predict([[30, 4.5, 3, 230]])\n",
    "print(a[0])\n",
    "clf.score(data, recommend_buy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Please download the zip file after running the cell below, then upload the zip file to GradeScope for submission. You can also download your notebook as an IPYNB file for the submission. Please also export your notebook as a PDF file (Use **Command/Control + P** if you have issues with the native export as PDF feature). **Please upload and submit both the IPYNB file and the PDF via Gradescope (entry code: GEWXGD).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "otter": {
   "tests": {
    "p4-1": {
     "name": "p4-1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(linear_regression(data[:100])) == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(\n...     linear_regression(data[:100]),\n...     np.array([4.18571357e-01, -6.56173031e-04,  2.26581560e-03,  1.02865569e-05]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p4-2": {
     "name": "p4-2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> predict([0.3, 0.002, 0.006, 0.00001], [3, 5.5, 60]) == 0.375\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p4-3": {
     "name": "p4-3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(bootstrap_sampling(data), pd.core.frame.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

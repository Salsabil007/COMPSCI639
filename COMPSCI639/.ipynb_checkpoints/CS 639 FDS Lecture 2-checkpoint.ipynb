{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Concentration Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will study one of the most important probabilistic results for learning: concentration inequalities. Concentration inequalities are as foundational as it gets: they give us the predictability of the data that allows us to learn. They also tell us how many samples we need to be confident about the prediction we are making. We will see some examples of learning and testing tasks that we can solve using these inequalities in the next lecture.\n",
    "\n",
    "We will start with the simplest concentration inequality: Markov Inequality. Then we will build on this basic inequality to obtain other inequalities such as the concentration inequalities Chebyshev, Chernoff, and Hoeffding. There are other inequalities too that we will not cover and some of them even apply to matrices! If you are curious to learn more, search for \"Bernstein's inequalities.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov inequality is the simplest concentration inequality, which should be your 'go-to' tool when you don't know much about the distribution of your random variable/data. \\textbf{It only applies to non-negative random variables}, so make sure you are not trying to apply it to a random variable that can take negative values. \n",
    "\n",
    "Given a non-negative random variable $X$ with finite expectation $\\mathbb{E}[X]$ and $t > 0$, Markov Inequality states that\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}[X \\geq t] \\leq \\frac{\\mathbb{E}[X]}{t}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inequality can be proved in only a few lines. It requires using that for non-negative random variables, it holds that\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[X] = \\int_0^{\\infty} \\mathbb{P}[X \\geq x]\\mathrm{d} x,\n",
    "\\end{equation*}\n",
    "\n",
    "(The proof can be found in basic probability texts and even on the Wikipedia page for the expected value. For a discrete random variable taking values in $\\{0, 1, 2, \\dots\\}$, you would use $\\mathbb{E}[X] = \\sum_{k=1}^{\\infty}\\mathbb{P}[X \\geq k].$ It is also possible to write the proof for a discrete random variable taking any discrete values; we might do this proof in class or in one of the discussion sessions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(!) Before looking at the derivation below, try to derive Markov Inequality on your own using what I have just told you!\n",
    "\n",
    "Given $t > 0$ and a non-negative random variable $X$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}[X] &= \\int_0^{\\infty} \\mathbb{P}[X \\geq x]\\mathrm{d} x\\\\\n",
    "    &\\geq \\int_0^{t} \\mathbb{P}[X \\geq x]\\mathrm{d} x\\\\\n",
    "    &\\geq \\int_0^{t} \\mathbb{P}[X \\geq t]\\mathrm{d} x\\\\\n",
    "    &= t {P}[X \\geq t].\n",
    "\\end{align*}\n",
    "\n",
    "Thus we can conclude that $\\mathbb{P}[X \\geq t] \\leq \\frac{\\mathbb{E}[X]}{t}$. We can also equivalently write Markov Inequality as\n",
    "$$\n",
    "    \\mathbb{P}[X \\geq t \\mathbb{E}[X]] \\leq \\frac{1}{t}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{example} Going back to our example of Bernoulli random variables, consider $X \\sim \\mathrm{Bernoulli}(p)$ for $p \\in (0, 1).$ This random variable only takes two values $-$ 0 and 1. As we discussed in the previous lecture, $\\mathbb{E}[X] = p.$ Markov Inequality gives us an estimate $\\mathbb{P}[X \\geq t p] \\leq \\frac{1}{t},$ for $t > 0.$\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why I gave the example of a Bernoulli random variable. After all, we know such random variables only take values 0 and 1 and $p$ already tells us what the probability of each of these values is. As it turns out, the example above is the simplest example that shows that Markov Inequality is in fact tight (provided all you know about your random variable is that it is non-negative). To see this, for any $t > 1$, consider the case of a random variable $X \\sim \\mathrm{Bernoulli}(p)$ with $p = 1/t$. Then, based on the example above, Markov inequality tells us that \n",
    "$$\n",
    "\\mathbb{P}[X \\geq t p] = \\mathbb{P}[X \\geq 1] = \\mathbb{P}[X = 1] \\leq \\frac{1}{t} = p.\n",
    "$$ \n",
    "\n",
    "It is also possible to scale this example and obtain an alternative (though quite similar) tight example.\n",
    "\n",
    "\\begin{example}\n",
    "Consider a random variable $X_k$ that, for some fixed $k > 1$, takes value 0 with probability $1 - \\frac{1}{k^2}$ and value $k$ with probability $\\frac{1}{k^2}.$ The expected value of this random variable is $\\frac{1}{k}.$ Markov inequality predicts that $(\\mathbb{P}[X_k = \\frac{1}{k}]=) \\mathbb{P}[X_k \\geq \\frac{1}{k}] \\leq \\frac{1/k}{k} = \\frac{1}{k^2},$ which holds with equality.\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov inequality is often used to estimate the fraction of high scores or, more generally values that are much higher than the mean. \n",
    "\n",
    "\\begin{example}\n",
    "Suppose that on our upcoming homework the class average is 80%. What is the highest proportion of students who can score 95+%?\n",
    "\n",
    "Let $X$ denote the score of a randomly chosen student. Applying Markov inequality, we get that $\\mathbb{P}[X \\geq 95] \\leq \\frac{80}{95} = \\frac{16}{19}\\approx 0.84.$ So at most 84% of the class can score 95% or higher. When would this average be achieved though?\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the previous example, it might seem unlikely that 84% of the class scores over 95% on the homework that has an average of 80% (though it's not impossible). Now let's consider the following case.\n",
    "\n",
    "\\begin{example}\n",
    "Same as in the previous example, assume that the class average on the homework is 80%, but now the minimum anyone scores is 60%. Then it is possible to obtain a tighter estimate of what proportion of students score 95% and higher by observing that $Y = X - 60$ is non-negative, where $X$ is the random variable corresponding to the score. Our estimate using Markov inequality now becomes $\\mathbb{P}[Y \\geq 35] \\leq \\frac{\\mathbb{E}[Y]}{35} = \\frac{\\mathbb{E}[X] - 60}{35} = \\frac{20}{35} = \\frac{4}{7} \\approx 0.57.$ So in this case at most 57% of the class can score 95% or higher. \n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should expect, Markov inequality can also be quite loose, as illustrated in the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{example}\n",
    "Consider the event of tossing a fair coin 10 times and getting all heads. Of course, we know the probability of this event; it is $\\big(\\frac{1}{2}\\big)^{10}.$ This probability is slightly lower than in one in a thousand. Now let us consider the estimate we would have gotten by applying Markov inequality. The expected number of heads for a fair coin in 10 coin tosses is 5. (Recall that we can define each coin toss as a separate random variable $X_i$ which takes value 1 if we get heads and zero otherwise. Then the total number of heads is the random variable equal to the sum $S_n = X_1 + X_2 + \\dots X_n$ with $n = 10$.) Markov inequality estimates the probability of 10 heads as $\\frac{5}{10} = \\frac{1}{2},$ which is over 500 times higher than the true probability $\\big(\\frac{1}{2}\\big)^{10}$ that we already computed. \n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should already be asking yourself whether making more assumptions about our random variables could give us tighter inequalities. Since we are still talking about concentration inequalities, the answer should clearly be 'yes' :)\n",
    "\n",
    "The first approach to tightening our concentration inequality is by assuming that our random variable $X$, in addition to having finite mean, also has finite variance. With this assumption, we get Chebyshev inequality, which states that for any $t > 0$,\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}[|X - \\mathbb{E}[X]| \\geq t] \\leq \\frac{\\mathrm{Var}[X]}{t^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to derive Chebyshev inequality directly from Markov inequality. To see this, notice that the variance is defined by\n",
    "\n",
    "$$\n",
    "    \\mathrm{Var}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2].\n",
    "$$\n",
    "\n",
    "So let us define a new random variable $Y = (X - \\mathbb{E}[X])^2$. Clearly, $Y$ is non-negative, and thus Markov inequality applies. \n",
    "\n",
    "Applying Markov inequality to $Y$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}[Y \\geq t^2] \\leq \\frac{\\mathbb{E}[Y]}{t^2}.\n",
    "\\end{align*}\n",
    "\n",
    "But, as we have already noted, $Y = (X - \\mathbb{E}[X])^2$ , and so the right-hand side of the above inequality is the same as the right-hand side of the stated Chebyshev inequality. So it only remains to argue that the left-hand side is the same. This follows simply from the definition of $Y$, as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}[Y \\geq t^2] &= \\mathbb{P}[(X - \\mathbb{E}[X])^2 \\geq t^2]\\\\\n",
    "    &= \\mathbb{P}[|X - \\mathbb{E}[X]| \\geq t].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{example}\n",
    "Let us go back to our example of tossing a fair coin 10 times and getting 10 heads. The variance in this case is $\\frac{10}{4} = 2.5$ (why?). Chebyshev inequality thus predicts that the probability of tossing exactly 10 heads is bounded by $\\frac{2.5}{5^2} = 0.1.$ This is still not tight, but it is much better than the $0.5$ bound we got from Markov inequality.\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can expect based on the discussion we had about Markov inequality, Chebyshev inequality is also tight if the only thing we are assuming about a random variable is that it has bounded mean $\\mu$ and variance $\\sigma^2$. This is illustrated in the following example.\n",
    "\n",
    "\\begin{example}\n",
    "Consider the random variable $X$ that takes value $-k$ with probability $\\frac{1}{2k^2},$ $+k$ with probability $\\frac{1}{2k^2}$, and $0$ with probability $1 - \\frac{1}{k^2}.$ The mean and the variance of this random variable are $\\mu = 0$ and $\\sigma^2 = 1.$ Chebyshev inequality predicts the following bound for $X$:\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}[|X - \\mu| \\geq  k] = \\mathbb{P}[|X| \\geq k] \\leq \\frac{1}{k^2},\n",
    "$$\n",
    "\n",
    "which holds with equality, as $\\mathbb{P}[|X| \\geq k] = \\mathbb{P}[|X| = k] = \\mathbb{P}[X = k] + \\mathbb{P}[X = -k] = \\frac{1}{k^2}.$\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev inequality can also be used to get a more quantifiable law of large numbers (and also prove the WLLN we stated last time). In particular, for $n$ i.i.d. random variables $X_1, X_2, \\dots, X_n$ with mean $\\mu$ and variance $\\sigma^2$, we have that \n",
    "\n",
    "$$\n",
    "    \\mathbb{E}\\Big[\\frac{X_1 + X_2 + \\dots + X_n}{n}\\Big] = \\mu, \\quad\\quad\\quad \\mathrm{Var}\\Big[\\frac{X_1 + X_2 + \\dots + X_n}{n}\\Big] = \\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "Applying Chebyshev inequality, for any $\\epsilon > 0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}\\Big[\\Big|\\frac{X_1 + X_2 + \\dots + X_n}{n} - \\mu\\Big| \\geq \\epsilon\\Big] \\leq \\frac{\\sigma^2}{n \\epsilon^2}. \n",
    "\\end{align*}\n",
    "\n",
    "Thus, for any $\\epsilon > 0$, we can take $n$ large enough to make the probability that the empirical mean deviates from the true mean by more than $\\epsilon$ arbitrarily small. Chebyshev inequality actually gives us more than that. It tells us that if we want to make the probability that the empirical and true mean differ by more than $\\epsilon$ at most $\\delta$ (for some $\\delta > 0$), then it suffices to take $n \\geq \\frac{\\sigma^2}{\\epsilon^2 \\delta}.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us observe that the Chebyshev inequality gives us the following estimate of how likely it is for a random variable to take values that fall more than $k$ standard deviations away from the mean. Applying Chebyshev inequality with $t = k\\sigma$, we have\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}[|X - \\mu| \\geq k \\sigma] \\leq \\frac{\\sigma^2}{k^2 \\sigma^2} = \\frac{1}{k^2}.\n",
    "$$\n",
    "\n",
    "Is this good? Bad? Well, it depends... As we stated before, we cannot do better if all we know about a random variable is that it has finite mean and finite variance. But Chebyshev inequality is quite loose for random variables with \"light tails,\" such as the Gaussian random variables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

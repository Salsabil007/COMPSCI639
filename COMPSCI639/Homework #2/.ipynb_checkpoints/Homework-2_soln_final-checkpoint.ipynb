{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Homework #2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 639 - Foundations of Data Science\n",
    "\n",
    "# Homework \\#2\n",
    "\n",
    "\n",
    "The goal of this homework is to provide practice examples for applications of concentration inequalities in determining confidence intervals and for performing simple hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 1: Empirical Estimate of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the discussion session (and Eric's notes) you saw how to estimate variance from bounded data samples when the true mean is known. In this question, you are asked to estimate the varaince in the same setting, but *without knowing the true mean.* The precise problem statement is:\n",
    "\n",
    "Suppose you are given access to i.i.d. samples $X_1, X_2, \\dots, X_n$ that are guaranteed to come from a probability distribution such that $X_i \\in [a, b]$ surely, for all $i \\in \\{1, \\dots, n\\}$, where $a < b$ are real numbers. \n",
    "\n",
    "Let $\\bar{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i$ be the empirical estimate of the mean and $\\bar{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{\\mu})^2$ be the empirical estimate of the variance. Given a confidence parameter $\\delta \\in (0, 1)$ and $n$ (the number of samples), provide a symmetric confidence interval for your variance estimate (that is, find $\\epsilon > 0$ such that your variance estimate is within $\\pm \\epsilon$ of the true variance with probability at least $1 - \\delta$). Justify your answer.\n",
    "\n",
    "Code a function in Python that takes $a, b, n, \\delta$ as input and returns $\\epsilon$ from your answer above.\n",
    "\n",
    "\\textbf{Hint:} Triangle inequality tells us that for any $p, q, r \\in \\mathbb{R}:$ $|p - q| \\leq |p - r| + |r - q|.$ Use triangle inequality and the union bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let, empirical variance = $\\hat{\\sigma}^2$ and empirical estimate of variance = $\\bar{\\sigma}^2$ and population variance = ${\\sigma}^2$\n",
    "\n",
    "$\\mathbb{P}{(|\\bar{\\sigma}^2 - {\\sigma}^2| \\geq k)} \n",
    "\\leq \\mathbb{P}{(|\\bar{\\sigma}^2 - \\hat{\\sigma}^2|+|\\hat{\\sigma}^2 - {\\sigma}^2| \\geq k)}\n",
    "= \\mathbb{P}{(|\\bar{\\sigma}^2 - \\hat{\\sigma}^2| \\geq k/2)} + \\mathbb{P}{(|\\hat{\\sigma}^2 - {\\sigma}^2| \\geq k/2)}$\n",
    "\n",
    "\n",
    "$\\mathbb{P}{(|\\hat{\\sigma}^2 - {\\sigma}^2| \\geq k/2)} \\leq 2e^{\\frac{-nk^2}{2(b-a)^4}}$ (From lecture, applying hoeffding bound)\n",
    "\n",
    "$\\bar{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{\\mu})^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i -\\mu + \\mu - \\bar{\\mu})^2 = \\hat{\\sigma}^2 - (\\mu - \\bar{\\mu})^2$\n",
    "or, $\\hat{\\sigma}^2 - \\bar{\\sigma}^2 = (\\mu - \\bar{\\mu})^2$\n",
    "\n",
    "==> $\\mathbb{P}{(|\\bar{\\sigma}^2 - \\hat{\\sigma}^2| \\geq k/2)} = \\mathbb{P}{(|(\\bar\\mu - \\mu)^2| \\geq k/2)} = \\mathbb{P}{(|\\bar\\mu - \\mu| \\geq \\sqrt{k/2})} \\leq 2e^{\\frac{-2nk}{2}} = 2e^{-nk}$ (Hoeffding bound)\n",
    "\n",
    "$\\mathbb{P}{(|\\bar{\\sigma}^2 - {\\sigma}^2| \\geq k)} \n",
    "\\leq \\mathbb{P}{(|\\bar{\\sigma}^2 - \\hat{\\sigma}^2| \\geq k/2)} + \\mathbb{P}{(|\\hat{\\sigma}^2 - {\\sigma}^2| \\geq k/2)}\n",
    "\\leq 2e^{-nk} + 2e^{\\frac{-nk^2}{2(b-a)^4}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 2: DNA and P-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strand of a DNA molecule consists of a sequence of four chemical bases: adenine (A), guanine (G), cytosine (C), and thymine (T). In a usual strand of human DNA, adenine and thymine comprise 30% of the chemical bases (each), while guanine and cytosine comprise the remaining 20% (each). Suppose you are presented with a sequence of $N$ letters A, G, C, and T that are claimed to come from a human DNA. Suppose that there are $n$ occurences of C in this sequence. You are asked to determine whether the sequence you see is surprising.\n",
    "\n",
    "1. Write down what are your null hypothesis and the alternative hypothesis.\n",
    "\n",
    "2. Write down what test statistic you would use.\n",
    "\n",
    "3. Write down the formula for the p-value for this specific problem.\n",
    "\n",
    "4. Assuming $N = 30$ and $n = 14,$ compute the p-value (you can do this in Python; you are allowed to use scipy.stats). Now use Chebyshev inequality and Hoeffding bound to bound the p-value. \n",
    "\n",
    "5. Using the computed bounds on the p-value (including the true p-value), discuss when you would reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "$H_0$ : The sequence is not surprizing, $H_1$ : The sequence is surprizing\n",
    "2.\n",
    "Test statisctic, $S$: number of occurrence of C in the sequence (=n)\n",
    "3.\n",
    "For two-sided test,\n",
    "p-value = $\\mathbb{P}[S \\geq n | H_0] + \\mathbb{P}[S \\leq n_1 | H_0]$, Here $n_1 = \\mathbb{E}[X] - (n - \\mathbb{E}[X])$ which is the left point that is equally distant from the expected value as the observed value, $n$ ( n_1 in this case is negative. so $\\mathbb{P}[S \\leq n_1 | H_0]$ adds nothings in this case for N=30, n = 14)\n",
    "4.\n",
    "The p value computed is 0.0009018.\n",
    "N = 30, n = 14, p = 0.20\n",
    "\n",
    "Let, $S_n$ be the number of occurrence of C. \n",
    "$S_n = X_1 + X_2 + X_3 .... + X_{30}$, where\n",
    "$X_i = 1$ if ith sequence is C ans 0 otherwise.\n",
    "$\\mathbb{E}[S_n]$ = Np = 30 * 0.20 = 6 and $var[S_n]$ = Npq = 30 * 0.20 * 0.80 = 4.8\n",
    "\n",
    "Chebyshev Inequality:\n",
    "$\\mathbb{P}[S_n >= 14] = \\mathbb{P}[|S_n - 6| \\geq 8] \\leq 4.8/64 = 0.075$\n",
    "\n",
    "Hoeffding bound:\n",
    "$\\mathbb{P}[S_n >= 14] = \\mathbb{P}[|\\frac{S_n}{30} - \\frac{6}{30}| \\geq \\frac{8}{30}] \\leq 2e^{\\frac{-2nt^2}{(b-a)^2}} = 2e^{\\frac{-64}{15}} = 0.028$, Here $[a,b] = [0,1]$\n",
    "\n",
    "5.\n",
    "Actual P value, P_ac = 0.00090, P_chebyshev $\\leq$ 0.075, P_hoeffding $\\leq$ 0.028.\n",
    "\n",
    "if $\\alpha$ = 0.05, we will reject $H_0$ for actual p value and P_hoeffding. But we will fail to reject $H_0$ for P_chebyshev.\n",
    "\n",
    "if $\\alpha$ = 0.01, we will reject $H_0$ for actual p value. But we will fail to reject $H_0$ for P_chebyshev and P_hoeffding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009018693287275339"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from scipy import stats\n",
    "\n",
    "#stats.binom_test(14, n=30, p=0.2, alternative='greater')\n",
    "stats.binom_test(14, n=30, p=0.2, alternative='two-sided')\n",
    "#stats.binom_test(14, n=30, p=0.2, alternative='less')\n",
    "#binom.pmf(0, 30, .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Problem 3: Detecting Cancer from DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact Sciences is a molecular diagnostics company specializing in the detection of early stage cancers based in Madison. Suppose you work for Exact Sciences and are tasked to develop a new diagnostic test for identifying whether a certain type of cancer is present in a patient based on the patient's DNA features. In particular, given a patient's DNA data encoded in a vector $x$ in $\\{-1, +1\\}^d$ where $d = 100$, you want to develop a test to classify whether the patient has cancer or not, outputting $-1$ when the patient does not have cancer and $+1$ when the patient has cancer.\n",
    "\n",
    "\n",
    "### Cancer Test Development Phase (Training Phase)\n",
    "\n",
    "You have $n$ patients' DNA features in a CSV file named `training-features.csv`. After conducting a 5-year long experiement, you also know whether the corresponding patients have developed cancer or not, and the labels are saved in another CSV file named `training-labels.csv` which will be what your diagnostic test aims to output.\n",
    "\n",
    "We provide an example code for training linear classifers ([link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)).\n",
    "\n",
    "```\n",
    "np.random.seed(0)  # Fixing a random seed\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[-1, +1], [+1, -1], [+1, +1], [-1, -1]]  # Training features\n",
    "Y = [-1, 1, 1, -1]  # Training labels\n",
    "\n",
    "# Define the classifer to train\n",
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Train the classifer\n",
    "clf.fit(X, Y)\n",
    "```\n",
    "\n",
    "1. Write a function that returns a trained linear classifer object using the training data provided. You should use `SGDClassifier` from `sklearn.linear_model` and train it with max number of iterations `max_ter = 1000` and tolerance `tol = 1e-3`. You should also initiate random seed to $0$ by calling `np.random.seed(0)` before training your classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to read training data into notebook.\n",
    "# Please put the csv files in the same directory as this notebook.\n",
    "\n",
    "training_features = np.genfromtxt(\"./training-features.csv\", delimiter=\",\")\n",
    "training_labels = np.genfromtxt(\"./training-labels.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier(training_features, training_labels):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_features: numpy.array\n",
    "        Features of the training dataset. Shape = (n, d)\n",
    "    training_labels: numpy.array\n",
    "        Labels of the training dataset. Shape = (n,)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    SGDClassifier\n",
    "        A trained classifier.\n",
    "        \n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    clf = SGDClassifier(max_iter=1000, tol=1e-4)\n",
    "    clf.fit(training_features, training_labels)\n",
    "    return clf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>p3-1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "p3-1 results: All test cases passed!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"p3-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2. How large does $n$ need to be so that with confidence 95% you can guarantee that\n",
    "$$\\mathbb{P}_{(x, y)\\sim \\mathcal{D}} [h(x) \\neq y] \\le \\epsilon$$ for $\\epsilon = 1e-3$, where $h$ is a linear classifer (a linear treshold function as in Lecture 4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 95% confidence, $\\delta = 0.05, \\epsilon = 1e - 3$,\n",
    "$n \\geq \\frac{2 (cd\\ln{d}\\ln{2} + \\ln{(2/\\delta)})}{\\epsilon^2} = \\frac{2 (100\\ln{100}\\ln{2} + \\ln{(2/0.05)})}{(1e-3)^2} = 8136.95 = 8137$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "3. Now suppose that you cannot recruit as many patients as in part 2, but you have $n$ patients, where $n$ is fixed. How small can you make $\\epsilon$? Make this value specific for $n = 8000$ (the number of examples in the training data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon^2 \\geq \\frac{2 (cd\\ln{d}\\ln{2} + \\ln{2/\\delta})}{n} \\approx \\frac{2 (d\\ln{d}\\ln{2} + \\ln{(1/\\delta)})}{n}$\n",
    "\n",
    "if n = 8000, length of vector d = 100, we can minimize $\\epsilon$ by making $\\ln(1/\\delta)$ = 1 or $\\delta$ = 1.\n",
    "\n",
    "So,  we get, $\\epsilon$ = 0.2824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "4. Given that $d = 100,$ discuss whether it would be possible to train a linear classifier with zero error (in terms of the population loss) in your lifetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want zero error, we need a linear function that minimizes the loss, $\\epsilon$ to 0.\n",
    "$|\\hat{L}(f) - L(f)| = \\epsilon = 0$. In that case, \n",
    "$n \\geq \\frac{2 (cd\\ln{d}\\ln{2} + \\ln{(2/\\delta)})}{\\epsilon^2}$ indicates that if $\\epsilon = 0$, n needs to be as large as infinity, which should not be possible I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Cancer Test Trial Phase (Testing Phase)\n",
    "\n",
    "After we have developed the diagnostic test for detecting cancer in patients, we typically evaluate how well the test performs in practice by going through a trial phase which involves using the test on real patients. A patient comes in and after conducting examinations to retrieve the patient's DNA features, you run the cancer diagnostic test to make a prediction of whether cancer has been detected.\n",
    "\n",
    "In this trial phase, we have $N$ patients and their DNA features are recorded in a CSV file named `testing-features.csv`. Assume we also get the true positive/negative cancer results via a more expensive and invasive diagnostic test, and the results are located in a CSV named `testing-labels.scv`.\n",
    "\n",
    "5. Write a function that makes predictions for the patients in the trial phase using the linear classifier you trained, then compute the following quantities for our cancer diagnostic test: 1) Specificity, 2) Sensitivity and 3) Empirical Misclassification Error. Your code should return a tuple of the three quantities in the same ordering.\n",
    "\n",
    "Hint: You can call `clf.predict([x])` to make a prediction for input feature `[x]` using your linear classifier `clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the classifier into variable diagnostic and read testing data into notebook.\n",
    "# Please put the csv files in the same directory as this notebook.\n",
    "\n",
    "diagnostic = train_classifier(training_features, training_labels)  # Your linear classifier\n",
    "\n",
    "testing_features = np.genfromtxt(\"./testing-features.csv\", delimiter=\",\")\n",
    "testing_labels = np.genfromtxt(\"./testing-labels.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_quantities(diagnostic, testing_features, testing_labels):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    diagnostic: SGDClassifier\n",
    "        A trained SGDClassifier linear classifier.\n",
    "    testing_features: numpy.array\n",
    "        Features of the testing dataset. Shape = (N, d)\n",
    "    testing_labels: numpy.array\n",
    "        Labels of the testing dataset. Shape = (N,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (float, float, float)\n",
    "        A tuple of specificity, sensitivity, miss.\n",
    "    \"\"\"\n",
    "    X = testing_features\n",
    "    y_actual = testing_labels\n",
    "    y_hat = diagnostic.predict(X)\n",
    "    #print(y_hat)\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    miss = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i] and y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i] and y_hat[i]==-1:\n",
    "           TN += 1\n",
    "        if y_hat[i]==-1 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "        if y_hat[i] != y_actual[i]:\n",
    "            miss += 1\n",
    "    #print(TP,FP,FN,TN,miss)\n",
    "    spec = TN / (TN + FP)\n",
    "    sens = TP/(TP+FN)\n",
    "    \n",
    "    #print(spec,sens,miss/ X.shape[0])\n",
    "    return (spec,sens,miss/X.shape[0])\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>p3-5</pre> results:</strong></p><p><strong><pre style='display: inline;'>p3-5 - 1</pre> result:</strong></p><pre>    Trying:\n",
       "        compute_performance_quantities(diagnostic, testing_features[:500], testing_labels[:500]) == (1.0, 0.9919028340080972, 0.004)\n",
       "    Expecting:\n",
       "        True\n",
       "    **********************************************************************\n",
       "    Line 1, in p3-5 0\n",
       "    Failed example:\n",
       "        compute_performance_quantities(diagnostic, testing_features[:500], testing_labels[:500]) == (1.0, 0.9919028340080972, 0.004)\n",
       "    Expected:\n",
       "        True\n",
       "    Got:\n",
       "        1.0 0.9919028340080972 0.004\n",
       "        True\n",
       "</pre>"
      ],
      "text/plain": [
       "p3-5 results:\n",
       "    p3-5 - 1 result:\n",
       "        Trying:\n",
       "            compute_performance_quantities(diagnostic, testing_features[:500], testing_labels[:500]) == (1.0, 0.9919028340080972, 0.004)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in p3-5 0\n",
       "        Failed example:\n",
       "            compute_performance_quantities(diagnostic, testing_features[:500], testing_labels[:500]) == (1.0, 0.9919028340080972, 0.004)\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            1.0 0.9919028340080972 0.004\n",
       "            True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"p3-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "6. Compute the specificity, sensitivity, and misclassification error using the above function you wrote for our diagnostic test. How does this compare to $\\epsilon$ you computed in Part 3? Output $\\epsilon_{\\mathrm{miss}} / \\epsilon$, where $\\epsilon_{\\mathrm{miss}}$ is the empirical misclassification error computed by your code, and discuss the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_{miss} = 0.004$ and $\\epsilon = 0.2824$. \n",
    "$\\epsilon_{miss}/\\epsilon$ = 0.01416\n",
    "It indicates that the calculated $\\epsilon$ is many times higher than the empirical $\\epsilon_{miss}$. We used Hoeffding bound to bound the $n$. And the Hoeffding bound is comparatively loose than actual. Therefore, we are getting smaller empirical $\\epsilon_{miss}$ than the $\\epsilon$ from our calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Please download the zip file after running the cell below, then upload the zip file to GradeScope for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "otter": {
   "tests": {
    "p3-1": {
     "name": "p3-1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> clf = train_classifier(training_features, training_labels)\n>>> isinstance(clf, SGDClassifier)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> clf = train_classifier(training_features, training_labels)\n>>> w = clf.coef_\n>>> theta = clf.intercept_\n>>> \n>>> # Check for approx value\n>>> expect_w = np.array([[ 2.82640291,  1.56384729, -3.37159738,  0.07173611,  1.67862508,\n...         -1.59254174,  0.64562503,  0.15781945,  0.04304167, -0.30129168,\n...          0.84648615,  2.65423623, -2.02295843,  2.16643065,  0.61693058,\n...          0.90387504, -2.33859733,  0.24390279,  3.25681959, -2.36729178,\n...         -4.2611252 , -3.8880974 ,  0.58823614,  5.15065301,  2.36729178,\n...          3.65854183,  0.12912501,  3.11334736, -0.67431948,  1.30559728,\n...         -0.58823614, -1.27690284,  0.27259723, -1.04734727,  2.96987514,\n...          0.38737502,  0.8751806 , -0.78909726,  2.74031957, -1.47776396,\n...          1.53515285, -0.73170837, -1.96556953, -1.01865282, -1.76470841,\n...         -0.44476391, -0.8177917 ,  0.73170837,  1.30559728, -0.30129168,\n...         -0.04304167,  0.70301392, -1.1908195 ,  1.53515285,  1.8794862 ,\n...          0.41606946,  5.23673635,  1.93687509, -0.21520834, -0.64562503,\n...         -0.53084725, -1.24820839,  2.36729178, -1.62123619, -0.41606946,\n...         -1.07604172,  2.13773621, -3.08465292, -0.61693058, -0.44476391,\n...          0.61693058,  0.96126393, -0.53084725,  2.13773621,  0.58823614,\n...          5.35151413, -3.25681959,  1.36298617, -2.39598622, -1.90818064,\n...          4.2611252 ,  3.91679185, -3.94548629, -0.35868057, -0.15781945,\n...         -4.74893077, -0.04304167,  1.01865282,  0.58823614, -4.46198632,\n...          2.25251399,  0.47345836, -0.1865139 , -1.62123619,  1.59254174,\n...          1.93687509, -1.10473616,  0.15781945,  1.76470841, -4.28981964]])\n>>> expect_theta = np.array([-0.08561223])\n>>> \n>>> np.allclose(w, expect_w) and np.allclose(theta, expect_theta)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p3-5": {
     "name": "p3-5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> compute_performance_quantities(diagnostic, testing_features[:500], testing_labels[:500]) == (1.0, 0.9919028340080972, 0.004)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
